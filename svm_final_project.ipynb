{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python390jvsc74a57bd0b75d60f7b8f1f6e8d8b4d74debe5a530c1607e49c5d7ca676d4bc427fe3a675e",
   "display_name": "Python 3.9.0 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "b75d60f7b8f1f6e8d8b4d74debe5a530c1607e49c5d7ca676d4bc427fe3a675e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# <center>** SVM - Support Vector Machine ** </center>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "notes for running the scripts: <br>\n",
    "python >= 3.8 <br>\n",
    "numpy==1.18.0 <br>\n",
    "pandas==1.2.3 <br>\n",
    "scikit-learn==0.24.1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets, svm as sk_svm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Kernel Trick\n",
    "def linear_kernel(x1, x2):\n",
    "    return np.dot(x1, x2)\n",
    "\n",
    "\n",
    "# for Kernel Trick\n",
    "def polynomial_kernel(x1, x2, p=3):\n",
    "    return (1 + np.dot(x1, x2)) ** p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM:\n",
    "    image_counter = 0  # used to save plots\n",
    "\n",
    "    def __init__(self, kernel=linear_kernel, C=1.0, tolerance=0.001, max_passes=10, random_state=None,\n",
    "                 visualization=False, plot_plane=True):\n",
    "        self.kernel = kernel  # the default uses no kernel (linear kernel)\n",
    "        self.C = C  # allows margin (C=1) to margins are heavily fined (C=100)\n",
    "        self.tolerance = tolerance  # tolernace for convergence\n",
    "        self.max_passes = max_passes  # also a tolernace for convergence\n",
    "        self.alphas = None  # aka lambdas - as stated in the dual problem\n",
    "        self.kernel_results = None\n",
    "        self.random_state = random_state\n",
    "        self.visualization = visualization  # flag for plotting\n",
    "        if self.visualization:\n",
    "            self.fig = plt.figure()\n",
    "            self.ax = self.fig.add_subplot(1, 1, 1)\n",
    "\n",
    "        self.plot_plane = plot_plane\n",
    "\n",
    "    def calc_kernel(self, X):\n",
    "        self.kernel_results = np.zeros((X.shape[0], X.shape[0]))\n",
    "\n",
    "        for i in range(X.shape[0]):\n",
    "            for j in range(X.shape[0]):\n",
    "                self.kernel_results[i, j] = self.kernel(X[i], X[j])\n",
    "\n",
    "    def calc_f(self, x, b):\n",
    "        f = 0.0\n",
    "        for i in range(self.X.shape[0]):\n",
    "            f += self.alphas[i] * self.y[i] * self.kernel(self.X[i, :], x)\n",
    "        f += b\n",
    "        return f\n",
    "\n",
    "    @staticmethod\n",
    "    # as part of the smo algorithm - alphaj(lambda_j) is selected randomly\n",
    "    def choose_j_randomly(i, n_samples, random_state):\n",
    "        j = i\n",
    "        while j == i:\n",
    "            j = random_state.randint(0, n_samples - 1)\n",
    "        return j\n",
    "\n",
    "    # calc higher and lower bounds on alphaj\n",
    "    def calc_L_H(self, yi, yj, alphai, alphaj):\n",
    "        if yi == yj:\n",
    "            L = max(0.0, alphaj + alphai - self.C)\n",
    "            H = min(self.C, alphaj + alphai)\n",
    "        else:\n",
    "            L = max(0.0, alphaj - alphai)\n",
    "            H = min(self.C, self.C + alphaj - alphai)\n",
    "\n",
    "        return L, H\n",
    "\n",
    "    def calc_eta(self, xi, xj):\n",
    "        return 2 * self.kernel(xi, xj) - self.kernel(xi, xi) - self.kernel(xj, xj)\n",
    "\n",
    "    # separating hyper plane calculation\n",
    "    def calc_w_star(self):\n",
    "        return sum([self.alphas[i] * self.y[i] * self.X[i, :] for i in range(self.X.shape[0])])\n",
    "\n",
    "    # fitting is done according to SMO\n",
    "    def fit(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "        n_samples, n_features = X.shape\n",
    "        self.alphas = np.zeros(n_samples)\n",
    "        b = 0.0\n",
    "        passes = 0\n",
    "        it = 0\n",
    "\n",
    "        while passes <= self.max_passes:\n",
    "            num_changed_alphas = 0\n",
    "            for i in range(n_samples):\n",
    "                f = self.calc_f(X[i, :], b)\n",
    "                Ei = f - y[i]\n",
    "                if (y[i] * Ei < -self.tolerance and self.alphas[i] < self.C) or (\n",
    "                        y[i] * Ei > self.tolerance and self.alphas[i] > 0.0):\n",
    "                    j = SVM.choose_j_randomly(i, n_samples, self.random_state)\n",
    "                    f = self.calc_f(X[j, :], b)\n",
    "                    Ej = f - y[j]\n",
    "                    alpha_i_old, alpha_j_old = self.alphas[i], self.alphas[j]\n",
    "                    L, H = self.calc_L_H(y[i], y[j], self.alphas[i], self.alphas[j])\n",
    "                    if abs(L - H) < 1e-4:\n",
    "                        continue\n",
    "                    eta = self.calc_eta(X[i, :], X[j, :])\n",
    "                    if eta >= 0:\n",
    "                        continue\n",
    "\n",
    "                    # calc alpha_j\n",
    "                    self.alphas[j] -= y[j] * (Ei - Ej) / eta\n",
    "                    if self.alphas[j] > H:\n",
    "                        self.alphas[j] = H\n",
    "                    elif self.alphas[j] < L:\n",
    "                        self.alphas[j] = L\n",
    "\n",
    "                    if abs(self.alphas[j] - alpha_j_old) < 1e-4:\n",
    "                        continue\n",
    "\n",
    "                    self.alphas[i] += y[i] * y[j] * (alpha_j_old - self.alphas[j])\n",
    "                    b1 = b - Ei - y[i] * (self.alphas[i] - alpha_i_old) * self.kernel(X[i, :], X[i, :]) - y[j] * (\n",
    "                            self.alphas[j] - alpha_j_old) * self.kernel(X[i, :], X[j, :])\n",
    "                    b2 = b - Ej - y[i] * (self.alphas[i] - alpha_i_old) * self.kernel(X[i, :], X[j, :]) - y[j] * (\n",
    "                            self.alphas[j] - alpha_j_old) * self.kernel(X[j, :], X[j, :])\n",
    "\n",
    "                    if 0 < self.alphas[i] < self.C:\n",
    "                        b = b1\n",
    "                    elif 0 < self.alphas[j] < self.C:\n",
    "                        b = b2\n",
    "                    else:\n",
    "                        b = (b1 + b2) / 2\n",
    "\n",
    "                    num_changed_alphas += 1\n",
    "            print(f'{it=}:{self.alphas=}')\n",
    "            it += 1\n",
    "\n",
    "            if num_changed_alphas == 0:\n",
    "                passes += 1\n",
    "            else:\n",
    "                passes = 0\n",
    "\n",
    "            if self.visualization:\n",
    "                self.w_star = self.calc_w_star()\n",
    "                self.b_star = b\n",
    "                self.visualize()\n",
    "\n",
    "        self.w_star = self.calc_w_star()\n",
    "        self.b_star = b\n",
    "\n",
    "        if self.visualization:\n",
    "            self.visualize(show=True)\n",
    "\n",
    "    # label prediction\n",
    "    def predict(self, x):\n",
    "        return np.sign(self.b_star + sum(\n",
    "            [self.alphas[i] * self.y[i] * self.kernel(self.X[i, :], x) for i in range(self.X.shape[0])]))\n",
    "\n",
    "    # plotting\n",
    "    def plot_2d_data(self):\n",
    "        target_plus_indicies = np.where(self.y == 1)[0]\n",
    "        target_minus_indicies = np.where(self.y == -1)[0]\n",
    "        sv_indicies = np.where(self.alphas > 0)[0]\n",
    "        self.ax.plot(self.X[target_plus_indicies, 0], self.X[target_plus_indicies, 1], 'o', label=1)\n",
    "        self.ax.plot(self.X[target_minus_indicies, 0], self.X[target_minus_indicies, 1], 'o', label=-1)\n",
    "        self.ax.plot(self.X[sv_indicies, 0], self.X[sv_indicies, 1], 'o', markersize=14, markerfacecolor=\"None\",\n",
    "                     label='sv')\n",
    "        plt.legend()\n",
    "        self.ax.set_xlabel('x1')\n",
    "        self.ax.set_ylabel('x2')\n",
    "        self.ax.axis('equal')\n",
    "        # plt.show()\n",
    "\n",
    "    # plotting\n",
    "    def visualize(self, show=False):\n",
    "        self.ax.cla()\n",
    "\n",
    "        self.plot_2d_data()\n",
    "        min_feature_value = np.min(self.X)\n",
    "        max_feature_value = np.max(self.X)\n",
    "        self.ax.set_xlim(min_feature_value, max_feature_value)\n",
    "        self.ax.set_ylim(min_feature_value, max_feature_value)\n",
    "\n",
    "        # hyperplane = x.w+b\n",
    "        # v = x.w+b\n",
    "        # psv = 1\n",
    "        # nsv = -1\n",
    "        # dec = 0\n",
    "        def hyperplane(x, w, b, v):\n",
    "            return (-w[0] * x - b + v) / w[1]\n",
    "\n",
    "        datarange = (min_feature_value * 0.9, max_feature_value * 1.1)\n",
    "        hyp_x_min = datarange[0]\n",
    "        hyp_x_max = datarange[1]\n",
    "\n",
    "        if self.plot_plane:\n",
    "            # (w.x+b) = 1\n",
    "            # positive support vector hyperplane\n",
    "            psv1 = hyperplane(hyp_x_min, self.w_star, self.b_star, 1)\n",
    "            psv2 = hyperplane(hyp_x_max, self.w_star, self.b_star, 1)\n",
    "            self.ax.plot([hyp_x_min, hyp_x_max], [psv1, psv2], 'k')\n",
    "\n",
    "            # (w.x+b) = -1\n",
    "            # negative support vector hyperplane\n",
    "            nsv1 = hyperplane(hyp_x_min, self.w_star, self.b_star, -1)\n",
    "            nsv2 = hyperplane(hyp_x_max, self.w_star, self.b_star, -1)\n",
    "            self.ax.plot([hyp_x_min, hyp_x_max], [nsv1, nsv2], 'k')\n",
    "\n",
    "            # (w.x+b) = 0\n",
    "            # positive support vector hyperplane\n",
    "            db1 = hyperplane(hyp_x_min, self.w_star, self.b_star, 0)\n",
    "            db2 = hyperplane(hyp_x_max, self.w_star, self.b_star, 0)\n",
    "            self.ax.plot([hyp_x_min, hyp_x_max], [db1, db2], 'y--')\n",
    "\n",
    "        self.plot_contours(cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "\n",
    "        precounter = ''\n",
    "        for i in range(3 - len(str(SVM.image_counter))):\n",
    "            precounter += '0'\n",
    "\n",
    "        plt.title('iteration ' + str(SVM.image_counter))\n",
    "        if not show:\n",
    "            plt.savefig('image_' + precounter + str(SVM.image_counter) + '.png')\n",
    "            SVM.image_counter += 1\n",
    "            plt.pause(0.01)\n",
    "        else:\n",
    "            plt.savefig('image_' + precounter + str(SVM.image_counter) + '.png')\n",
    "            SVM.image_counter += 1\n",
    "            plt.show()\n",
    "\n",
    "    # plotting\n",
    "    def make_meshgrid(self, h=0.02):\n",
    "        x_min, x_max = np.min(self.X[:, 0]) - 1.5, np.max(self.X[:, 0]) + 1.5\n",
    "        y_min, y_max = np.min(self.X[:, 1]) - 1.5, np.max(self.X[:, 1]) + 1.5\n",
    "        xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "        return xx, yy\n",
    "\n",
    "    # plotting\n",
    "    def plot_contours(self, **params):\n",
    "        xx, yy = self.make_meshgrid()\n",
    "        Z = np.array([self.predict([xx_i, yy_i]) for xx_i, yy_i in zip(xx, yy)])\n",
    "        # Z = self.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        out = self.ax.contourf(xx, yy, Z, **params)\n",
    "        return out"
   ]
  },
  {
   "source": [
    "the model: find best seperating hyperplane seperator between 2 classes.<br>\n",
    "best in the sense of maximizing the minimal hyperplane seperator distance.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<center><img src=\"images/svm_general_4.png\"></center>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The seperating hyperplane can be written as the set of points $\\vec x$ satisfiying:\n",
    "$$(1)\\; \\vec w^T \\cdot \\vec x  +b = 0$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# mathematical formulation:\n",
    "$$(2)\\;f(\\vec x,\\vec w,b)=sign(\\vec w^T \\cdot \\vec x +b)$$\n",
    "$$(3)\\;Margin( \\vec w ,b) = min Margin(x_i)$$\n",
    "$$(4)\\;(\\vec w,b) = argmax Margin(\\vec w ,b)$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<center><img src=\"images/svm_2.png\"></center>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Let's define $$(5)\\;n=\\dfrac {\\vec w} {||\\vec w||}$$ $$(6)\\;\\vec w^T \\cdot \\vec {x^+}  +b = 1$$ $$(7)\\;\\vec w^T \\cdot \\vec {x^-}  +b = -1$$\n",
    "(6) and (7) are called the suppurt vectors. <br>\n",
    "note the the decision boundary remains: $\\vec w^T \\cdot \\vec x  +b = 0$. <br> \n",
    "so given a new vector $\\vec x_i$ the class will be determined as stated in (2): $sign(\\vec w^T \\cdot \\vec x_i +b)$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "so the margin we want to maximize is:\n",
    "$$(8)\\; Margin = (x^+ - x^-) \\cdot n = (x^+ - x^-) \\cdot \\dfrac {\\vec w} {||\\vec w||}$$\n",
    "applying (5) to (8):\n",
    "$$(9)\\;  Margin = (x^+ - x^-) \\cdot \\dfrac {\\vec w} {||\\vec w||}$$\n",
    "applying (6) and (7) to (8):\n",
    "$$(10)\\; Margin = (\\dfrac {1-b} {\\vec w} - \\dfrac {-1-b} {\\vec w})\\cdot \\dfrac {\\vec w} {||\\vec w||} =\\dfrac 2 {||\\vec w||}$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Maximizing $\\dfrac 2 {||w||}$ can be expressed as Minimizing: $$(11)\\;\\dfrac 1 2 ||\\vec w||^2$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Let's define $y_i \\in \\{1,-1\\}$ to be the class label of $\\vec x_i$, $i=\\{1,...,n\\}$. <br>\n",
    "note that the constraints (6) and (7) can be written as:\n",
    "$$(12)\\;y_i(\\vec w^T \\cdot \\vec {x}  +b)-1 \\geq 0$$\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "**finalizing the formulation:**<br>\n",
    "Minimizing: $$\\dfrac 1 2 ||\\vec w||^2$$\n",
    "subject to:\n",
    "$$y_i(\\vec w^T \\cdot \\vec {x}  +b)-1 \\geq 0$$\n",
    "The above formulation is an optimization problem with a convex quadratic objective and linear constraints, it can be solved using QP solvers."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Hard Margin"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "the above formulation does not allow errors.<br>\n",
    "let's define a hyperplane and randomly draw point.<br>\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_1():\n",
    "    # EXAMPLE 1\n",
    "    random_state = np.random.RandomState(0)\n",
    "    X = random_state.randn(10, 2)\n",
    "    b = -0.2\n",
    "    w = np.array([0.5, -0.3])\n",
    "    y = np.sign(b + np.dot(X, w))\n",
    "\n",
    "    svm = SVM(C=100.0, random_state=random_state, visualization=False)\n",
    "    svm.fit(X, y)\n",
    "\n",
    "example_1()"
   ]
  },
  {
   "source": [
    "<center><img src=\"images/example_1_c100_new_2.gif\"></center>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "the final result:\n",
    "<center><img src=\"images/example_1_C_100_final_new.png\"></center>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Lagrange Duality"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The general Lagrangian function:\n",
    "$$ (13)\\;\\mathcal{L}(x,\\lambda,\\nu) = f_0(x)+ \\sum_{i=1}^{m} \\lambda_i f_i(x)+\\sum_{i=1}^{p} \\nu_i h_i()$$\n",
    "applying it to our case, where:\n",
    "$$(14)\\;f_0(w) = \\dfrac 1 2 ||\\vec w||^2$$\n",
    "$$(15)\\;f_1(w,b) = -y_i(\\vec w^T \\cdot \\vec {x_i}  +b)+1 \\leq 0$$\n",
    "$$(16)\\; \\mathcal{L}(w,b,\\lambda) = \\dfrac 1 2 ||\\vec w||^2-\\sum_{i=1}^{m}\\lambda_i \\{y_i(\\vec w^T \\cdot \\vec {x_i}  +b)+1\\}$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "we'll use the KKT conditions:\n",
    "$$(17)\\; \\dfrac{\\partial{}}{\\partial w} \\mathcal{L}(w,b,\\lambda)=w-\\sum_{i=1}^{m}\\lambda_i y_i x_i = 0$$\n",
    "so:\n",
    "$$(18)\\;w=\\sum_{i=1}^{m}\\lambda_i y_i x_i $$\n",
    "$$(19)\\; \\dfrac{\\partial{}}{\\partial b} \\mathcal{L}(w,b,\\lambda)=\\sum_{i=1}^{m}\\lambda_i y_i = 0$$\n",
    "\n",
    "from (13) and (18):\n",
    "\n",
    "$$(20)\\; \\mathcal{L}(w,b,\\lambda)= \\sum_{i=1}^{m}\\lambda_i - \\dfrac 1 2 \\sum_{i,j=1}^{m} y_i y_j \\lambda_i \\lambda_j \\vec x_i ^ T \\vec x_j - b\\sum_{i,j=1}^{m} \\lambda_i y_i$$\n",
    "\n",
    "adding (19) to (20):\n",
    "$$(21)\\; \\mathcal{L}(w,b,\\lambda)= \\sum_{i=1}^{m}\\lambda_i - \\dfrac 1 2 \\sum_{i,j=1}^{m} y_i y_j \\lambda_i \\lambda_j \\vec x_i ^ T \\vec x_j$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "the dual optimization problem can be expressed as Maximizing:\n",
    " $$  \\mathcal{L}(\\lambda)= \\sum_{i=1}^{m}\\lambda_i - \\dfrac 1 2 \\sum_{i,j=1}^{m} y_i y_j \\lambda_i \\lambda_j \\langle\\vec x_i, \\vec x_j\\rangle  $$\n",
    " s.t.\n",
    " $$ \\lambda_i \\geq 0, i=1,...,m$$\n",
    " $$\\sum_{i=1}^{m}\\lambda_i y_i = 0$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "note that after finding the $\\lambda$s we need to go back to the primal problem and find $w^*$ and $b^*$. <br>\n",
    "$w^*$ can be extracted directly from (18).\n",
    "$$(22)\\; b^* = -\\dfrac 1 2 \\{max_{i:y_i=-1} w^{*^T} x_i + min_{i:y_i=1} w^{*^T}x_i \\} $$\n",
    "also note that now (2) can be expressed as:\n",
    "$$ (23)\\;f(\\vec x,\\vec w,b)=sign(\\vec w^T \\cdot \\vec x +b) = sign((\\sum_{i=1}^{m}\\lambda_i y_i x_i)^T\\vec x +b)=sign(\\sum_{i=1}^{m}\\lambda_i y_i \\langle \\vec x_i, \\vec x \\rangle +b)$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "let's examine it using the well-known iris dataset (taking into account only 2 labels and 2 features)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_2():\n",
    "    ## EXAMPLE 2\n",
    "    iris = datasets.load_iris()\n",
    "    X = iris['data'][:, (2, 3)]\n",
    "    scaler = StandardScaler()\n",
    "    Xstan = scaler.fit_transform(X)\n",
    "\n",
    "    data = pd.DataFrame(data=Xstan, columns=['petal length', 'petal width'])\n",
    "    data['target'] = iris['target']\n",
    "    data = data[data['target'] != 2]  # we will only focus on Iris-setosa and Iris-Versicolor\n",
    "    data['target'].iloc[data['target'] == 0] = -1\n",
    "\n",
    "    random_state = np.random.RandomState(0)\n",
    "    svm = SVM(C=100.0, random_state=random_state, visualization=False)\n",
    "    svm.fit(data.loc[:, ['petal length', 'petal width']].values, data['target'].values)\n",
    "\n",
    "example_2()"
   ]
  },
  {
   "source": [
    "<center><img src=\"images/example_2_C_100_new.gif\"></center>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "the final result:\n",
    "<center><img src=\"images/example_2_C_100__final_new.png\"></center>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Soft Margin"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "what will we do when we have inconsistent data (some outlier)?\n",
    "Let's take the first example, and libertly flip a label"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_1_1():\n",
    "    # EXAMPLE 1_1\n",
    "    random_state = np.random.RandomState(0)\n",
    "    X = random_state.randn(10, 2)\n",
    "    b = -0.2\n",
    "    w = np.array([0.5, -0.3])\n",
    "    y = np.sign(b + np.dot(X, w))\n",
    "    y[3] = -y[3]\n",
    "\n",
    "    svm = SVM(C=100.0, random_state=random_state, visualization=False)\n",
    "    svm.fit(X, y)\n",
    "\n",
    "example_1_1()"
   ]
  },
  {
   "source": [
    "<center><img src=\"images/example_1_1_C_100_new.gif\"></center>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "the solution: allow slacks \n",
    "<br>\n",
    "\n",
    "we can change (11) to satisfy our new need:<br> \n",
    "Minimizing: $$(24)\\;\\dfrac 1 2 ||\\vec w||^2 + C \\sum_{i=1}^{m} \\xi_i$$\n",
    "subject to:\n",
    "$$(25)\\;y_i(\\vec w^T \\cdot \\vec {x}  +b)\\geq 1-\\xi_i$$\n",
    "$$(26)\\;\\xi_i \\geq 0$$\n",
    "<br>\n",
    "$C$ controls the relative weighting between the twin goals of making ||\\vec w||^2 small and of ensuring that most examples have functional margin at least 1.\n",
    "\n",
    "<br>\n",
    "reforming the lagrangian yields:\n",
    "\n",
    "$$(27)\\; \\mathcal{L}(w,b,\\lambda) = \\dfrac 1 2 w^T w+C\\sum_{i=1}^{m}\\xi_i-\\sum_{i=1}^{m}\\lambda_i \\{y_i(\\vec w^T \\cdot \\vec {x_i}  +b)-1+\\xi_i\\}-\\sum_{i=1}^{m}r_i \\xi_i$$\n",
    "\n",
    "<br>\n",
    "\n",
    "$\\lambda_i$ and $r_i$ are the lagrangian multipliers.\n",
    "\n",
    "<br>\n",
    "after derivation we get:\n",
    "\n",
    "$$(28)\\; \\mathcal{L}(w,b,\\lambda)= \\sum_{i=1}^{m}\\lambda_i - \\dfrac 1 2 \\sum_{i,j=1}^{m} y_i y_j \\lambda_i \\lambda_j \\vec x_i ^ T \\vec x_j$$\n",
    "\n",
    "s.t.\n",
    " $$(29)\\;  0 \\leq \\lambda_i \\leq C, i=1,...,m$$\n",
    " $$(30)\\; \\sum_{i=1}^{m}\\lambda_i y_i = 0$$\n",
    " <br>\n",
    " \n",
    " $w^*$ can again be extracted directly from (18).\n",
    " <br>\n",
    "\n",
    " note that KKT conditions:\n",
    " $$(31)\\;  \\lambda_i=0: y_i(\\vec w^T \\cdot \\vec {x_i}  +b) \\geq 1$$\n",
    " $$(32)\\;  \\lambda_i=C: y_i(\\vec w^T \\cdot \\vec {x_i}  +b) \\leq 1$$\n",
    " $$(33)\\;  0<\\lambda_i<C: y_i(\\vec w^T \\cdot \\vec {x_i}  +b) = 1$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Let's view the Iris example again with a changed label:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_2_1():\n",
    "    ## EXAMPLE 2_1\n",
    "    iris = datasets.load_iris()\n",
    "    X = iris['data'][:, (2, 3)]\n",
    "    scaler = StandardScaler()\n",
    "    Xstan = scaler.fit_transform(X)\n",
    "\n",
    "    data = pd.DataFrame(data=Xstan, columns=['petal length', 'petal width'])\n",
    "    data['target'] = iris['target']\n",
    "    data = data[data['target'] != 2]  # we will only focus on Iris-setosa and Iris-Versicolor\n",
    "    data['target'].iloc[data['target'] == 0] = -1\n",
    "    # data = data.loc[(0,1,2,3,4,90,91,92,93,94),:]\n",
    "    data['target'].iloc[20] = -data['target'].iloc[20]\n",
    "\n",
    "    random_state = np.random.RandomState(0)\n",
    "    svm = SVM(C=100.0, random_state=random_state, visualization=False)\n",
    "    svm.fit(data.loc[:, ['petal length', 'petal width']].values, data['target'].values)\n",
    "\n",
    "example_2_1()"
   ]
  },
  {
   "source": [
    "<center><img src=\"images/example_2_1_C_100_new.gif\"></center>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "now, changing to C=1.0:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_2_2():\n",
    "    ## EXAMPLE 2_2\n",
    "    iris = datasets.load_iris()\n",
    "    X = iris['data'][:, (2, 3)]\n",
    "    scaler = StandardScaler()\n",
    "    Xstan = scaler.fit_transform(X)\n",
    "\n",
    "    data = pd.DataFrame(data=Xstan, columns=['petal length', 'petal width'])\n",
    "    data['target'] = iris['target']\n",
    "    data = data[data['target'] != 2]  # we will only focus on Iris-setosa and Iris-Versicolor\n",
    "    data['target'].iloc[data['target'] == 0] = -1\n",
    "    # data = data.loc[(0,1,2,3,4,90,91,92,93,94),:]\n",
    "    data['target'].iloc[20] = -data['target'].iloc[20]\n",
    "\n",
    "    random_state = np.random.RandomState(0)\n",
    "    svm = SVM(C=1.0, random_state=random_state, visualization=False)\n",
    "    svm.fit(data.loc[:, ['petal length', 'petal width']].values, data['target'].values)\n",
    "\n",
    "example_2_2()"
   ]
  },
  {
   "source": [
    "<center><img src=\"images/example_2_1_C_1_new.gif\"></center>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "the SVM yields the wanted seperating plane despite the error."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# SMO - Sequential Minimal Optimization\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "after getting to the lagrangian formulation, we need to find the corresponding lagrange multipliers in order to get w, b in the primal problem.<br>\n",
    "\n",
    "Let's assume that we have a set of $\\lambda_i$ that satisfy (29) and (30).\n",
    "what if we could change one $\\lambda_i$ in order to increase $\\mathcal{L}$ but from (30):\n",
    "$$\\lambda_1 y_1 = -\\sum_{i=2}^{m}\\lambda_i y_i$$\n",
    "by multiplying by $y_i$:\n",
    "$$\\lambda_1 = -y_1 \\sum_{i=2}^{m}\\lambda_i y_i$$\n",
    "\n",
    "it's not possible since one change affects the other.\n",
    "\n",
    "the solution: <br>\n",
    "update 2 $\\lambda_i$s simultaneously in order to keep the constarints.\n",
    "\n",
    "while not converge{ <br>\n",
    "    &emsp; select $\\lambda_i$, $\\lambda_j$ (heuristicly towards global maximum) <br>\n",
    "    &emsp; optimize $\\mathcal{L}$ with corresponding $\\lambda_i$, $\\lambda_j$ (other $\\lambda$ remain fixed) <br>\n",
    "} <br>\n",
    "\n",
    "testing convergence: <br>\n",
    "check that (31),(32),(33) are within tolerance bounds."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Let's assume that we have a set of $\\lambda_i$ that satisfies (29),(30)and we chose  $\\lambda_1$, $\\lambda_2$ to optimize.<br>\n",
    "from (30): <br>\n",
    "$$\\lambda_1 y_1 + \\lambda_2 y_2 = -\\sum_{i=3}^{m}\\lambda_i y_i$$\n",
    "the right-hand side is fixed, so:\n",
    "$$(34)\\; \\lambda_1 y_1 + \\lambda_2 y_2 = \\zeta = const$$\n",
    "from (29) we know that $\\lambda_1$, $\\lambda_2$ lie within $[ 0,C ]$.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<center><img src=\"images/L_H_lambda.png\"></center>\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "so we have a lower boud L and upper bound H for $\\lambda_2$: <br>\n",
    "\n",
    "$$ L \\leq \\lambda_2 \\leq H $$\n",
    "\n",
    "we can rewrite (34) as:\n",
    "$$ (35)\\; \\lambda_1  = (\\zeta -\\lambda_2 y_2) y_1 $$\n",
    "\n",
    "and now also:\n",
    "$$\\mathcal{L}(\\lambda_1,\\lambda_2,...\\lambda_m) = \\mathcal{L}((\\zeta -\\lambda_2 y_2) y_1,\\lambda_2,...\\lambda_m)$$\n",
    "\n",
    "remember that $\\lambda_3,...\\lambda_m$ are consts. <br>\n",
    "we get a quadratic function in $\\lambda_2$ that can be expressed for some $a,b,c$:\n",
    "$$a\\lambda_2^2+b\\lambda_2+c$$\n",
    "to maximize the above we can derive and compare to 0, remembering we are bounded by L and H so:\n",
    "$$  \\lambda_2^{new} > H : \\lambda_2^{new} = H$$\n",
    "$$  \\lambda_2^{new} < H : \\lambda_2^{new} = L$$\n",
    "$$  L<\\lambda_2^{new} < H: \\lambda_2^{new}=\\lambda_2^{new}$$\n",
    "\n",
    "$\\lambda_1$ can be extracted from (35).\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "as for proof of convergence (credit to Yoni):<br> it is based on a theorem that shows the ability to decompose the QP into smaller QP problems, solutions of which guarantee the solution of the original QP. <br> reference to the theorem: <br> https://ieeexplore.ieee.org/document/622408"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "\n",
    "full SMO article:<br>\n",
    "https://www.microsoft.com/en-us/research/publication/sequential-minimal-optimization-a-fast-algorithm-for-training-support-vector-machines/"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Kernel"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "what about linearity? <br>\n",
    "in a simple example the SVM cannot find a linear seperator. <br>\n",
    "let's see the following example:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_3():\n",
    "    ## EXAMPLE 3\n",
    "    random_state = np.random.RandomState(0)\n",
    "    X, y = datasets.make_moons(noise=0.1, random_state=2)\n",
    "    y[y == 0] = -1\n",
    "    svm = SVM(C=1.0, random_state=random_state, visualization=True)\n",
    "    svm.fit(X, y)\n",
    "\n",
    "example_3()"
   ]
  },
  {
   "source": [
    "<center><img src=\"images/example_3_new.gif\"></center>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "no linear seperator - can converge (after a while) but is it what we're looking for?\n",
    "<center><img src=\"images/example_3_new_final.png\"></center>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Kernel to the rescue.<br>\n",
    "we can manipulate the data to a higher dimension, eventually - a linear seperator will exist.<br>\n",
    "we won't go in to the details, but the \"Kernel Trick\" allows us to \"enjoy\" the higher dimension with lower dimension calculations.<br>\n",
    "further details:<br>\n",
    "https://towardsdatascience.com/the-kernel-trick-c98cdbcaeb3f"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "we'll apply a polynomial kernel to the above example:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_4():\n",
    "    ## EXAMPLE 4\n",
    "    random_state = np.random.RandomState(0)\n",
    "    X, y = datasets.make_moons(noise=0.1, random_state=2)\n",
    "    y[y == 0] = -1\n",
    "    svm = SVM(C=1.0, kernel=gaussian_kernel, random_state=random_state, visualization=True, plot_plane=False)\n",
    "    svm.fit(X, y)\n",
    "\n",
    "example_4()"
   ]
  },
  {
   "source": [
    "<center><img src=\"images/example_4_new.gif\"></center>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "note that the colors represent the different classes, since we are not talking about 2d space."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Real DataSets"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "our first try was with facial expression recognition.<br>\n",
    "for further details, check:<br>\n",
    "https://www.kaggle.com/ashishpatel26/facial-expression-recognitionferchallenge"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<center><img src=\"images/fer.png\"></center>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The training set consists of 28,709 examples, with 48*48 pixels.<br>\n",
    "it appears that we were not up to the challnge, we only got around 57% accuracy. <br>\n",
    "(accuracy=0.57, precision=1.0, recall=0.0077, f1=0.01538)\n",
    "<br> <br>\n",
    "different things we tried to do:\n",
    "<br>\n",
    "1. slicing the examples <br>\n",
    "1. taking only half-picture <br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download fer2013.csv form the link above and rename it to train.csv\n",
    "def example_5():\n",
    "    ## EXAMPLE 5\n",
    "    df = pd.read_csv('train.csv')\n",
    "    df1 = df[np.logical_or(df['emotion'] == 3, df['emotion'] == 4)]\n",
    "    img_array = df1.pixels.apply(lambda x: np.array(x.split(' ')).reshape(48*48).astype('float32'))\n",
    "\n",
    "\n",
    "    X = np.stack(img_array, axis=0)\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "\n",
    "    # consider only the bottom half\n",
    "    # X = X[:, range(int(48*48/2), X.shape[1])]\n",
    "    # X = X[:, range(int(48 * 48 / 2))]\n",
    "    \n",
    "    y = df1['emotion'].values\n",
    "\n",
    "    y[y == 3] = -1\n",
    "    y[y == 4] = 1\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "    # X = img_array[range(100),:]\n",
    "    # y = df1['emotion'].values[range(100)]\n",
    "    X_train = X_train[range(500), :]\n",
    "    y_train = y_train[range(500)]\n",
    "    X_test = X_test[range(300), :]\n",
    "    y_test = y_test[range(300)]\n",
    "    random_state = np.random.RandomState(0)\n",
    "    svm = SVM(C=1.0, kernel=gaussian_kernel, random_state=random_state, visualization=False)\n",
    "    svm.fit(X_train, y_train)\n",
    "\n",
    "    # accuracy = 0\n",
    "    # for x, actual_y in zip(X_test, y_test):\n",
    "    #     pred_y = svm.predict(x)\n",
    "    #     if pred_y == actual_y:\n",
    "    #         accuracy += 1\n",
    "\n",
    "    # accuracy /= X_test.shape[0]\n",
    "    # print(f'{accuracy=}')\n",
    "    accuracy = 0\n",
    "    false_positive = 0\n",
    "    false_negative = 0\n",
    "    true_positive = 0\n",
    "    for x, actual_y in zip(X_test, y_test):\n",
    "        pred_y = svm.predict(x)\n",
    "        if pred_y == actual_y:\n",
    "            accuracy += 1\n",
    "            if pred_y == 1:\n",
    "                true_positive += 1\n",
    "        else:\n",
    "            if pred_y == 1:\n",
    "                false_positive += 1\n",
    "            else:\n",
    "                false_negative += 1\n",
    "\n",
    "    print('OUR SVM, using SMO:')\n",
    "    accuracy /= X_test.shape[0]\n",
    "    print(f'{accuracy=}')\n",
    "    precision = true_positive / (true_positive + false_positive)\n",
    "    print(f'{precision=}')\n",
    "    recall = true_positive / (true_positive + false_negative)\n",
    "    print(f'{recall=}')\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    print(f'{f1=}')\n",
    "    print('')\n",
    "\n",
    "example_5()"
   ]
  },
  {
   "source": [
    "so we went after another dataset, this time breast cancer.<br>\n",
    "link:<br>\n",
    "https://www.kaggle.com/uciml/breast-cancer-wisconsin-data\n",
    "<br><br>\n",
    "the taget is to calssify if a tummor is malignant or benign. <br>\n",
    "the data contains 30 featues and 569 instances split into 381 train instances and 188 test instances."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_6():\n",
    "    ## EXAMPLE 6\n",
    "    # download data.csv form the link above\n",
    "    df = pd.read_csv('data.csv')\n",
    "    X = df[['radius_mean', 'texture_mean', 'perimeter_mean',\n",
    "           'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n",
    "           'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n",
    "           'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n",
    "           'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n",
    "           'fractal_dimension_se', 'radius_worst', 'texture_worst',\n",
    "           'perimeter_worst', 'area_worst', 'smoothness_worst',\n",
    "           'compactness_worst', 'concavity_worst', 'concave points_worst',\n",
    "           'symmetry_worst', 'fractal_dimension_worst']].values\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "\n",
    "    y = df['diagnosis'].values\n",
    "    y[y=='M'] = -1\n",
    "    y[y=='B'] = 1\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "    random_state = np.random.RandomState(0)\n",
    "    svm = SVM(C=1.0, kernel=polynomial_kernel, random_state=random_state, visualization=False)\n",
    "    svm.fit(X_train, y_train)\n",
    "\n",
    "    accuracy = 0\n",
    "    false_positive = 0\n",
    "    false_negative = 0\n",
    "    true_positive = 0\n",
    "    for x, actual_y in zip(X_test, y_test):\n",
    "        pred_y = svm.predict(x)\n",
    "        if pred_y == actual_y:\n",
    "            accuracy += 1\n",
    "            if pred_y == 1:\n",
    "                true_positive += 1\n",
    "        else:\n",
    "            if pred_y == 1:\n",
    "                false_positive += 1\n",
    "            else:\n",
    "                false_negative += 1\n",
    "\n",
    "    print('OUR SVM, using SMO:')\n",
    "    accuracy /= X_test.shape[0]\n",
    "    print(f'{accuracy=}')\n",
    "    precision = true_positive / (true_positive + false_positive)\n",
    "    print(f'{precision=}')\n",
    "    recall = true_positive / (true_positive + false_negative)\n",
    "    print(f'{recall=}')\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    print(f'{f1=}')\n",
    "    print('')\n",
    "\n",
    "    y_train = y_train.astype('int')\n",
    "    clf = sk_svm.SVC(C=1.0, kernel='poly', random_state=42).fit(X_train, y_train)\n",
    "    accuracy = 0\n",
    "    false_positive = 0\n",
    "    false_negative = 0\n",
    "    true_positive = 0\n",
    "    for x, actual_y in zip(X_test, y_test):\n",
    "        pred_y = clf.predict(x.reshape(1, -1))\n",
    "        if pred_y == actual_y:\n",
    "            accuracy += 1\n",
    "            if pred_y == 1:\n",
    "                true_positive += 1\n",
    "        else:\n",
    "            if pred_y == 1:\n",
    "                false_positive += 1\n",
    "            else:\n",
    "                false_negative += 1\n",
    "\n",
    "    print('sklearn:')\n",
    "    accuracy /= X_test.shape[0]\n",
    "    print(f'{accuracy=}')\n",
    "    precision = true_positive / (true_positive + false_positive)\n",
    "    print(f'{precision=}')\n",
    "    recall = true_positive / (true_positive + false_negative)\n",
    "    print(f'{recall=}')\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    print(f'{f1=}')\n"
   ]
  },
  {
   "source": [
    "OUR SVM, using SMO: <br> <br>\n",
    "accuracy =  0.925531914893617 <br>\n",
    "precision = 0.9421487603305785 <br>\n",
    "recall =    0.9421487603305785 <br>\n",
    "f1 =        0.9421487603305785 <br>\n",
    "<br>\n",
    "we compared the result to sklearn's SVC, using the same parameters: <br>  <br>\n",
    "accuracy =  0.8882978723404256 <br>\n",
    "precision = 0.852112676056338 <br>\n",
    "recall =    1.0 <br>\n",
    "f1 =        0.9201520912547528 <br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}